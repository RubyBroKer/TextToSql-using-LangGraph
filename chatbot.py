import streamlit as st
import random
import time
#importing to create and interact with database and tables
import sqlite3
from typing import Annotated
from typing_extensions import TypedDict
from operator import add
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
#using hugging face llm model for the query generation
from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace
import os
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
os.environ["LANGCHAIN_API_KEY"] = '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'
os.environ["LANGCHAIN_TRACING_V2"]="true"
os.environ["LANGCHAIN_PROJECT"]="CourseLanggraph"
from huggingface_hub import login
login(token='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')

global connection
connection = sqlite3.connect("employee.db")
global cursor
cursor = connection.cursor()
llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct",
    task="text-generation",
    max_new_tokens=512,
    do_sample=False,
    repetition_penalty=1.03,
)

chat = ChatHuggingFace(llm=llm, verbose=True)
class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    prompt: list[str]
    schema : list[str]
    query : list[str]
    results: Annotated[list[str], add]

graph_builder = StateGraph(State)
#agent to get the table schema
def get_table_schema(state, table_name="employees"):
    schema_response = ""
    try:
        cursor = connection.cursor()
        query = f"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table_name}';"
        cursor.execute(query)
        result = cursor.fetchone()

        if result:
            schema_response = f"Schema for table '{table_name}':\n{result[0]}"
        else:
            schema_response = f"Table '{table_name}' does not exist."
    except sqlite3.Error as e:
        schema_response = f"An error occurred: {e}"
    finally:
        cursor.close()
    state['schema'] = [schema_response]

    return state
    # return state
# Add nodes (functions)
graph_builder.add_node("get_table_schema", get_table_schema)
# Define the edges between nodes
graph_builder.add_edge(START, "get_table_schema")
#agent to generate the query using llm
def query_gen(state: State):
    messages = [
    ("system", "Act as an MYSQL expert. And answer the prompt . Don't explain the answer, Just return the MYSQL query and nothing else."),
    ("human", state['prompt'][0]+state['schema'][0]),]
    h = chat.invoke(messages)
    print(h.content)
    state['query'] = [h.content]
    return state 
# Add nodes (functions)
graph_builder.add_node("query_gen", query_gen)
# Define the edges between nodes
graph_builder.add_edge("get_table_schema","query_gen")
#agent to execute the query generated by llm
def results_gen(state: State):
    # print(state)
    cursor.execute(state['query'][0])
    result = []

    for row in cursor.fetchall():
        result.append(row)
    return {"results": [result]}
    return state
# Add nodes (functions)
graph_builder.add_node("results_gen", results_gen)
# Define the edges between nodes
graph_builder.add_edge("query_gen","results_gen")
# Define the edges between nodes
graph_builder.add_edge("results_gen",END)
# Compile the graph
graph = graph_builder.compile()
# Streamed response emulator
def response_generator(user_input,results):

    for event in graph.stream({"prompt": [user_input]}):
        for value in event.values():
            results = value["results"]
    return results



st.title("Assistant")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept user input
if prompt := st.chat_input("What is up?"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)

    # Display assistant response in chat message container
    with st.chat_message("assistant"):
        response = st.write_stream(response_generator(prompt,[]))
    # Add assistant response to chat history
    st.session_state.messages.append({"role": "assistant", "content": response})
